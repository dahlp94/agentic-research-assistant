# Agentic Research & Decision Assistant (LangGraph)

A **production-oriented multi-agent AI system** that performs structured research and decision-making using explicit orchestration, safety checks, and traceable outputs.

This project is designed as an **Applied AI Engineer portfolio artifact**, emphasizing **reliability, debuggability, and system design** over prompt-only demos.

---

## ðŸ” What This System Does

Given an open-ended task (for example: *â€œCompare X vs Y and recommend oneâ€*), the system runs a **multi-agent workflow**:

* **Planner** â€” decomposes the task into research questions, deliverables, and a rubric
* **Researcher** â€” gathers evidence and produces structured claims with citations
* **Critic** â€” checks for unsupported claims, weak evidence, missing counterarguments, and unclear assumptions
* **Decider** â€” synthesizes a final recommendation with tradeoffs, confidence, and next steps

The system is intentionally **safety-first**: when evidence is insufficient, it lowers confidence or requests additional research instead of hallucinating.

---

## ðŸ§  Why LangGraph?

This project uses **LangGraph** to model agent execution as an explicit **graph / state machine**, rather than an opaque chain of prompts.

This enables:

* bounded retry loops (no runaway agents)
* deterministic routing logic
* inspectable intermediate state
* clean separation between agents

These properties are critical for **production agent systems**.

---

## ðŸ—ï¸ Architecture (High Level)

```
User Query
   â†“
Planner
  (task type, research questions, rubric, risks)
   â†“
Researcher
  (claims + citations)
   â†“
Critic
  â”œâ”€ if issues found â†’ targeted re-research (bounded)
  â””â”€ else
   â†“
Decider
  (recommendation, tradeoffs, confidence)
   â†“
Artifacts
  - JSON trace (full execution state)
  - Markdown report (user-facing output)
```

All agents operate over a **shared typed state**, making execution traceable and debuggable.

---

## ðŸ“ Repository Structure

```txt
agentic-research-assistant/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ ara/
â”‚       â”œâ”€â”€ api/                 # FastAPI service layer
â”‚       â”‚   â”œâ”€â”€ app.py
â”‚       â”‚   â””â”€â”€ schemas.py
â”‚       â”‚
â”‚       â”œâ”€â”€ agents/              # Role-specialized agents
â”‚       â”‚   â”œâ”€â”€ planner.py
â”‚       â”‚   â”œâ”€â”€ researcher.py
â”‚       â”‚   â”œâ”€â”€ critic.py
â”‚       â”‚   â””â”€â”€ decider.py
â”‚       â”‚
â”‚       â”œâ”€â”€ core/                # Orchestration & shared logic
â”‚       â”‚   â”œâ”€â”€ state.py         # Typed shared state (Pydantic)
â”‚       â”‚   â”œâ”€â”€ runner.py        # LangGraph wiring & execution
â”‚       â”‚   â”œâ”€â”€ policies.py      # Retry & safety policies
â”‚       â”‚   â””â”€â”€ tracing.py       # Artifact persistence
â”‚       â”‚
â”‚       â”œâ”€â”€ tools/               # External tool interfaces
â”‚       â”‚   â”œâ”€â”€ web_search.py
â”‚       â”‚   â””â”€â”€ citations.py
â”‚       â”‚
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_local.py             # CLI entry point
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ sample_runs/             # JSON traces & Markdown reports
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_state_machine.py
```

This structure separates **agent logic**, **orchestration**, and **infrastructure concerns**, making the system easier to extend, test, and reason about as complexity grows.

---

## ðŸš€ Quickstart

### Local CLI

```bash
pip install -e .
python scripts/run_local.py "Compare Redis vs Postgres for caching LLM outputs. Recommend one."
```

### API

```bash
uvicorn ara.api.app:app --reload --port 8000
```

POST `/run`:

```json
{
  "query": "Pick a vector database for a small team shipping RAG. Compare FAISS, Qdrant, and Pinecone."
}
```

Artifacts are written to `outputs/sample_runs/` as JSON and Markdown files.

---

## ðŸ›¡ï¸ Safety & Reliability Features

* **Typed shared state** (Pydantic)
* **Critic-driven validation** of evidence quality
* **Bounded retry loops** to prevent infinite agent cycles
* **Explicit failure modes** when evidence is insufficient
* **Deterministic artifacts** for auditing and debugging

This mirrors real-world constraints in enterprise AI systems.

---

## ðŸ§© Design Decisions

This project prioritizes **reliability, observability, and controlled execution** over raw generation quality.

### 1ï¸âƒ£ Explicit Agent Roles vs. Single â€œSuper-Agentâ€

Separate Planner, Researcher, Critic, and Decider agents improve debuggability and localize failures at the cost of additional orchestration complexity.

### 2ï¸âƒ£ LangGraph for Orchestration

Agent execution is modeled as an explicit graph/state machine to enable deterministic routing, bounded retries, and inspectable control flow.

### 3ï¸âƒ£ Typed Shared State (Pydantic)

All agents read/write to a structured state object, preventing schema drift and enabling artifact persistence and validation.

### 4ï¸âƒ£ Critic-Driven Validation

A dedicated Critic agent enforces evidence quality and can halt or redirect execution, reducing hallucinations.

### 5ï¸âƒ£ Bounded Retry Loops

Critique â†’ research cycles are explicitly limited to ensure predictable execution and bounded cost.

### 6ï¸âƒ£ Artifact-First Outputs

Every run produces a full JSON execution trace and a user-facing Markdown report for transparency and debugging.

### 7ï¸âƒ£ Placeholder Tooling Before Optimization

Tooling is stubbed initially to validate control flow before integrating real data sources.

### 8ï¸âƒ£ Safety Over Fluency

The system prefers low-confidence or insufficient-evidence outputs over confident but unsupported answers.

---

## ðŸ“¦ Project Status

**Implemented**

* Planner â†’ Researcher â†’ Critic â†’ Decider pipeline
* LangGraph orchestration with conditional routing
* CLI + FastAPI interface
* Run artifacts (JSON + Markdown)
* Safe failure behavior

**Planned / In Progress**

* Real web search integration (Tavily / Bing / SerpAPI)
* Citation quality scoring
* Cost & latency tracking per agent
* Offline evaluation harness
* Caching and model routing

---

## ðŸ§ª Development Notes

* Web search is currently stubbed.
* The Critic correctly flags placeholder citations.
* This behavior is intentional and demonstrates safe defaults.

---

## ðŸŽ¯ Who This Is For

This project is intended to demonstrate skills relevant to:

* Applied AI Engineer
* LLM Platform Engineer
* Agentic Systems Engineer

It focuses on **system design, safety, and reliability**, not prompt hacking.

---

## ðŸ“Œ Key Takeaway

> This repository demonstrates how to build **agentic AI systems that are safe, inspectable, and production-ready**, rather than brittle prompt pipelines.

---